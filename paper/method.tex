\section{Method}
\label{sec:method}

We will now elaborate on how we designed our interviews
(\Cref{sec:interviews}) and our online survey
(\Cref{sec:online-survey}).  For both we will discuss participant
recruitment, how we structured the interview or survey process, and research
ethics.

\subsection{Interviews}
\label{sec:interviews}

We developed a question set that served as the basis for each
interview.\footnote{The question set is available online at
\url{https://nymity.ch/onion-services/pdf/interview-checklist.pdf}.}  The
semi-structured nature of our interviews allowed us to deviate from this
question set, \eg, by asking follow-up questions.  We began by asking
demographic information (gender, age range, occupation, country of residence,
and level of education), followed by information about online behavior, and
finally questions specific to Tor Browser and onion services.

\subsubsection{Procedure}

Princeton University's institutional review board (\textsc{irb}) deemed our
study exempt from further review.\footnote{Our \textsc{irb} protocol number is
8251.}  We conducted thirteen interviews in person and four interviews remotely;
over Skype, Signal, WhatsApp, and Jitsi---depending on what our interviewees
felt the most comfortable with.  For in-person interviews we asked our
interviewees to sign a consent form.  This was not practical for remote
interviews, so we sent the consent form in advance, over email, and after
seeking permission from our \textsc{irb}, asked for verbal consent before the
interview.  In all cases we explicitly asked for permission to record the
conversation.  All except two participants agreed to have their interview
recorded.  In the remaining two interviews we took notes instead.  We made it
clear to our participants that they could withdraw their consent at any time.
As part of our interviews we asked our participants to draw sketches of how they
believed Tor and onion services worked.  Each interview ended with a debriefing
phase in which we asked if our participants had any remaining questions.  After
that we offered our participants a gift card worth twenty dollars as a token of
appreciation.  We conducted our first interview on July 13, 2017 and the last on
October 20, 2017.  The median interview time was 34:30 minutes.  Our shortest
interview lasted for 20:59 minutes while our longest interview took 50:56
minutes.

We had our recordings transcribed by a company that offered transcription
services and a non-disclosure agreement protected the confidentiality of our
data.  Once our interview recordings were transcribed, we deleted the original
recordings and employed qualitative data coding to analyze the transcripts.
Most interview transcripts were coded by two members of our team.  This process
identified twenty-six themes that are all listed in \Cref{sec:coding-themes}.

\subsubsection{Recruitment}

To select eligible interview subjects, we created a short pre-interview survey
(see \Cref{sec:interview-survey}), which was advertised by The Tor
Project both in a blog post~\cite{Winter2017a} and on its Twitter account.  Our
selection process favored laypeople and sought to maximize cultural, gender,
location, education, and age diversity.  In addition to our online screening, we
recruited participants in person at an Internet freedom event.  We found it
difficult to draw a uniform sample of Tor users to interview. We believe that
The Tor Project's blog and Twitter account are mainly followed by
disproportionately technical users while many non-technical users may install
Tor Browser in a one-off process and then cease to follow the project.  To make
matters worse, many Tor users value their privacy significantly more than the
average Internet user, making it challenging to evoke enough trust to have users
open up to us about their browsing habits.

We ended up interviewing seventeen subjects whose demographic information is
shown in \Cref{tab:interviewees}.  Given the sensitive nature of our
interviews, we only present aggregate information to protect the identity of our
participants.  We believe that our sample is biased towards educated and
technical users (almost 60\% of our participants have a postgraduate degree) but
it also shows the diversity among Tor's user base: Our participants comprised
human rights activists, legal professionals, writers, artists, and journalists,
just to name a few.

\begin{table*}[ht]
	\centering
	\caption{The distribution over gender, age, country of residence, and
	education for our seventeen interview subjects.  We chose not to display
	per-person demographic information to protect the identity of our interview
	subjects.}
	\label{tab:interviewees}
	\begin{tabular}{l r r | l r r | l r r | l r r}
	\toprule
	Age & \# & \% &
	Gender & \# & \% &
	Continent of residence & \# & \% &
	Education & \# & \% \\
	\midrule
	18--25 & 2  & 11.8 & Female & 5  & 29.4 & Asia          & 3 & 17.6 & No degree    & 1  & 5.9 \\
	26--35 & 10 & 58.8 & Male   & 12 & 70.6 & Australia     & 1 &  5.9 & High school  & 3  & 17.7 \\
	36--45 & 4  & 23.5 &        &    &      & Europe        & 4 & 23.5 & Graduate     & 3  & 17.7 \\
	46--55 & 1  & 5.9  &        &    &      & North America & 8 & 47.1 & Postgraduate & 10 & 58.8 \\
	       &    &      &        &    &      & South America & 1 &  5.9 & & & \\
	\bottomrule
	\end{tabular}
\end{table*}

\subsection{Online survey}
\label{sec:online-survey}

Shortly after we conducted our first batch of interviews, we launched an online
survey to complement our interview data.

\subsubsection{Procedure}

We created our survey in Qualtrics because our institution had a subscription,
it had all the features we deemed necessary, and an out-of-the-box Tor Browser
could display its interface correctly.  Qualtrics however requires JavaScript
which is deactivated if Tor Browser is set to its highest security setting.  A
number of users complained about our reliance on JavaScript in the recruitment
blog post comments~\cite{Winter2017a}.

Respondents had to agree to a consent form before starting the survey. The
consent form informed the respondents about the procedure of our experiment and
required that all respondents were at least eighteen years of age.  Our survey
was only available in English, but we targeted an international audience because
Sawaya \ea\ showed that there are cultural differences in security
behavior~\cite{Sawaya2017a}.  Ignoring these differences would tailor Tor
Browser to the needs of a predominantly Western audience which runs counter to
The Tor Project's global mission.

We used cognitive pretesting (sometimes also called cognitive interviewing) to
improve the wording of our survey questions~\cite{Collins2003a}.  Pretesting
reveals if respondents \first~understand questions, \second~understand questions
consistently, and \third~understand questions the way we intended.  A pretest
entailed administering our survey and asking our respondents to fill out the
survey while verbalizing their thought process.  We occasionally asked follow-up
questions to make sure that our pretesters understood all questions as intended.
However, not all cognitive processes can be verbalized and cognitive pretesting
may change the way respondents answer questions.  We had five pretesters whose
input helped us improved our survey iteratively.  Two pretesters were native
English speakers while the remaining three were fluent but spoke English as a
second language.

To weed out low-quality responses we incorporated four attention checks into our
survey~\cite{Berinsky2014a}.  Having more than one attention check 
allowed us to measure a respondent's \emph{degree} of attention, and we
discarded responses that failed more than two attention checks.

The majority of our survey focused on onion services, but we also added some
questions about Tor in general.  \Cref{tab:survey-structure} shows that our
survey consists of six blocks that are ordered by topic.  It takes about fifteen
minutes to answer all questions.  The full survey is listed in
\Cref{app:interview-questions}.

\begin{table}[t]
	\centering
	\caption{The topical question blocks in our survey and the number of
	questions they contain.}
	\label{tab:survey-structure}
	\begin{tabular}{l r}
	\toprule
	Topic & \# of questions \\
	\midrule
	Consent and demographic information & 1 \\
	Tor usage & 4 \\
	Onion site usage & 20 \\
	Onion site operation & 5 \\
	Onion site phishing and impersonation & 9 \\
	Expectations of privacy & 9 \\
	End of survey & 1 \\
	\midrule
	Total & 49 \\
	\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Recruitment}

Similar to our interviews, we advertised our survey \first~in a blog post on The
Tor Project's blog~\cite{Winter2017a}, \second~on its corresponding Twitter
account, and \third~on three Reddit subforums.\footnote{The forums are
\url{https://reddit.com/r/tor/}, \url{https://reddit.com/r/onions/}, and
\url{https://reddit.com/r/samplesize/}.}  Unlike our interview participants,
our survey respondents are self-selected.  Again, we expect this recruitment
strategy to bias our sample towards engaged users because casual Tor users are
unlikely to follow The Tor Project's social media accounts.

To incentivize participation, we originally planned to give respondents the
option to participate in a gift card lottery but we abandoned the idea because
it was difficult to reconcile anonymous participation with a lottery because we
would have to collect our respondents' email addresses to notify them in case
they won.  Despite the lack of incentives, we collected a satisfactory number of
responses.  In fact, we believe that many respondents were only motivated by
improving Tor---some of our interview participants even turned down the gift
card we offered them.

We launched our survey on August 16, 2017 and ended it on September 11, 2017, so
it was active for twenty-seven days and was taken 828 times.  However, not all
responses are necessarily of high quality; people may have rushed their answers,
aborted our survey prematurely, or given deliberately wrong answers.  We
therefore weed out low-quality responses that either did not finish the survey
or that failed more than two out of our four attention checks.  We collected a
total of 828 responses, but only 604 (73\%) completed the survey, and 527 (64\%)
passed at least two attention checks.  The remainder of this work focuses on
these 527 responses.

\Cref{tab:survey-demo} shows the demographics of our survey.  Not
surprisingly, our respondents were \emph{young and educated}: more than sixty
percent are younger than thirty-six, and another sixty percent have at least a
graduate degree.  Finally, another sixty percent consider themselves at least
highly knowledgeable in matters of Internet privacy and security.

\begin{table*}[t]
	\centering
	\caption{The distribution over gender, age, education, and domain knowledge
	for our 527 survey respondents.  It was optional to provide demographic
	information which is why we lack data for a small number of respondents.}
	\label{tab:survey-demo}
	\begin{tabular}{l r r | l r r | l r r | l r r}
	\toprule
	Gender & \# & \% &
	Age & \# & \% &
	Education & \# & \% &
	Domain knowledge & \# & \% \\
	\midrule
	Male   & 444 & 85.7 & 18--25 & 186 & 35.7 & No degree     &  26 & 5.0  & No knowledge             &   1 & 0.2  \\
	Female &  49 &  9.5 & 26--35 & 184 & 35.3 & High school   & 173 & 33.3 & Mildly knowledgeable     &  37 & 7.1  \\
	Other  &  25 &  4.8 & 36--45 &  88 & 16.9 & Graduate      & 215 & 41.4 & Moderately knowledgeable & 178 & 34.1 \\
	N/A    &   9 &  1.7 & 46--55 &  43 &  8.3 & Post graduate & 105 & 20.2 & Highly knowledgeable     & 230 & 44.1 \\
	       &     &      & 56--65 &  16 &  3.0 & N/A           &   8 &  1.5 & Expert                   &  76 & 14.6 \\
	       &     &      & $>$ 65 &   4 &  0.8 &               &     &      & N/A                      &   5 &  1.0 \\
	       &     &      & N/A    &   6 &  1.2 &               &     &      &                          &     & \\
	\bottomrule
	\end{tabular}
\end{table*}
